<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.3" />
<title>adversaries API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>adversaries</code> module</h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import json
import pickle
import re
import sys
import os
import datetime
from subprocess import run as run_cmd, PIPE

import click
import pandas as pd
from tqdm import tqdm

wd = os.getcwd() + &#39;/&#39;
sys.path.append(wd + &#39;../&#39;)
# Anchors
sys.path.append(wd + &#39;anchor/&#39;)
# LORE
sys.path.append(wd + &#39;lorem/&#39;)
# BRL
sys.path.append(wd + &#39;sklearn-expertsys/&#39;)

from anchor.anchor_tabular import AnchorTabularExplainer
from lorem.datamanager import prepare_dataset
from lorem.lorem import LOREM
from trepan.transform import trepan_to_rules
from trepan.trepan import TREPAN
import pysbrl

from pandas import DataFrame
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from keras.models import load_model

from numpy import array, argwhere, hstack, float64, int64, inf, rint, argmax

from discretize import EntropyDiscretizer
from rule_based_classifier import RuleBasedClassifier
# from RuleListClassifier import RuleListClassifier

from models import Rule

dt_hyperparameters = {
    &#39;max_depth&#39;: (2, 4, 8, 16, 32),
    &#39;min_samples_split&#39;: (.1, .2, .5, .75),
    &#39;min_samples_leaf&#39;: (1, 2, 5, 10, 25)
}


def __encode_for_jrbc(df):
    &#34;&#34;&#34;Encode @df for JRBC algoriths.

    Arguments:
        df (DataFrame): DataFrame to encode, including the labels.

    Returns:
        (tuple): Tuple (DataFrame, list, EntropyDiscretizer)
                holding the encoded dataset, a list of pairs
                (feature, label_encoder) and the discretizer.
    &#34;&#34;&#34;
    encoded_df = df.copy()
    data = encoded_df.values[:, :-1]
    labels = encoded_df.values[:, -1]
    types = encoded_df.dtypes

    non_discretized_features = argwhere(types != object).flatten().tolist()
    discretized_features = argwhere(types == object).flatten().tolist()
    feature_names = encoded_df.columns

    discretizer = None
    discretized_data = data
    if len(non_discretized_features) &gt; 0:
        discretizer = EntropyDiscretizer(data, discretized_features, feature_names, labels=labels.astype(int))
        discretized_data = discretizer.discretize(data)

    encoded_df = DataFrame(hstack((discretized_data, labels.reshape(-1, 1))), columns=encoded_df.columns)
    scaling_factors = [0]
    label_encoders = {}
    for feature in encoded_df.columns:
        label_encoder = LabelEncoder().fit(encoded_df[feature])
        encoded_df[feature] = label_encoder.transform(encoded_df[feature])
        encoded_df[feature] += sum(scaling_factors)
        scaling_factors.append(encoded_df[feature].unique().shape[0])
        label_encoders[feature] = label_encoder

    return encoded_df, label_encoders, discretizer, scaling_factors


def _decode_from_jrbc(df, encoders, discretizer, scaling_factors):
    &#34;&#34;&#34;Scale @x, @y from CPAR&#39;s encoding.
    Each feature is scaled from positive integers and with values larger then the preceding.
    Returns the data scaled back to its original encoding.
    &#34;&#34;&#34;
    df_ = df.copy()

    # Scale in order to have sorted rows
    for (i, encoder), scaling_factor in zip(encoders, scaling_factors):
        df_[i] = encoder.inverse_transform(df_[i] - scaling_factor)
    df_ = discretizer.undiscretize(df_)

    return df_


def _binary_discretization(df, buckets=4, name=None):
    &#34;&#34;&#34;Discretize the given `x` into binary `buckets` per feature.
    Args:
        df (DataFrame): The data.
        buckets (int): Number of buckets.
        name (str): If not None, dump discretization info on `name`.json
    Returns:
        (DataFrame): Binarly discretized DataFrame.
    &#34;&#34;&#34;
    m, n = df.shape
    columns = df.columns
    bucket_size = m // buckets
    x = df.values
    binned_x = df.copy().values[:, :-1]

    continuous_features = [i for i in range(n - 1) if len(set(x[:, i])) &gt; 2]
    discrete_features = [i for i in range(n - 1) if len(set(x[:, i])) &lt;= 2]
    buckets_argindices = [(bucket_size * i, bucket_size * (i + 1)) for i in range(buckets)]
    buckets_argindices[-1] = (buckets_argindices[-1][0], buckets_argindices[-1][1] - 1)
    full_thresholds = {}

    for feature in continuous_features:
        values = sorted(set(x[:, feature]))
        bucket_size = len(values) // buckets
        thresholds = [(int(values[i * bucket_size]), int(values[(i + 1) * bucket_size])) for i in range(buckets - 1)]
        full_thresholds[columns[feature]] = thresholds

        for bucket_id, (lower_bound, upper_bound) in zip(range(buckets), thresholds):
            binned_x[:, feature][(lower_bound &lt;= binned_x[:, feature]) &amp;
                                 (binned_x[:, feature] &lt; upper_bound)] = bucket_id
        full_thresholds[columns[feature]].append((full_thresholds[columns[feature]][-1][1],
                                                  int(binned_x[:, feature].max())))
        binned_x[:, feature][binned_x[:, feature] &gt; buckets] = buckets - 1

    binned_df = pd.DataFrame(binned_x, columns=columns[:-1])
    continuous_columns = [columns[i] for i in continuous_features]
    discrete_columns = [columns[i] for i in discrete_features]

    for feature in continuous_columns:
        binned_df[feature] = binned_df[feature].astype(int).astype(str)

    if n - 1 != len(discrete_features):
        binarized_x = pd.concat([pd.get_dummies(binned_df[continuous_columns], prefix_sep=&#39;=&#39;), binned_df[discrete_columns]],
                                axis=&#39;columns&#39;)
        binarized_x[&#39;y&#39;] = df.values[:, -1]
    else:
        binarized_x = pd.concat([pd.get_dummies(binned_df[continuous_columns]), df[&#39;y&#39;]], axis=&#39;columns&#39;)

    # Columns
    full_cols = []
    cols = binarized_x.columns.tolist()
    for col in cols:
        if &#39;_&#39; in col and col[:col.index(&#39;_&#39;)] in full_thresholds:
            feature, interval = int(col.split(&#39;=&#39;))
            interval = full_thresholds[feature][interval]
            full_cols.append(feature + &#39;=&#39; + str(interval))
        else:
            full_cols.append(col)
    binarized_x.columns = full_cols

    if name is not None:
        with open(name + &#39;.discretization.json&#39;, &#39;w&#39;) as log:
            json.dump(full_thresholds, log)

    return binarized_x


def _label_discretization(df, buckets=4, max_elements_per_feature=10):
    &#34;&#34;&#34;Discretize the given `x` into integer `buckets`.
    Args:
        df (DataFrame): The data.
        buckets (int): Number of buckets.
        max_elements_per_feature (int): All features with less than
                                        `max_elements_per_feature` are
                                        not discretized.
    Returns:
        (x): Labelized `x`.
    &#34;&#34;&#34;
    x = df.values
    m, n = x.shape
    bucket_size = m // buckets
    binned_x = x.copy()

    continuous_features_indices = [c for c in range(n - 1) if df.dtypes[c] != object and
                                   len(df[df.columns[c]].unique()) &gt; buckets]
    continuous_features = [df.columns[i] for i in continuous_features_indices]
    categorical_features = [df.columns.tolist()[c] for c in range(n - 1) if df.dtypes[c] == object]
    buckets_argindices = [(bucket_size * i, bucket_size * (i + 1)) for i in range(buckets)]
    buckets_argindices[-1] = (buckets_argindices[-1][0], buckets_argindices[-1][1] - 1)
    full_thresholds = {}

    for feature in continuous_features_indices:
        values = sorted(set(x[:, feature]))
        bucket_size = len(values) // buckets
        thresholds = [(values[i * bucket_size], values[(i + 1) * bucket_size]) for i in range(buckets - 1)]
        full_thresholds[feature] = thresholds

        for bucket_id, (lower_bound, upper_bound) in zip(range(buckets), thresholds):
            binned_x[:, feature][(lower_bound &lt;= binned_x[:, feature].astype(float64)) &amp;
                                 (binned_x[:, feature].astype(float64) &lt; upper_bound)] = str(bucket_id)
        binned_x[:, feature][binned_x[:, feature].astype(float64) &gt; bucket_size] = buckets - 1

    if len(categorical_features) &gt; 0:
        labelized_x = pd.concat([pd.DataFrame(binned_x[:, continuous_features_indices], columns=continuous_features),
                                 pd.get_dummies(df[categorical_features], prefix_sep=&#39;=&#39;),
                                 df.iloc[:, -1]],
                                axis=&#39;columns&#39;)
    else:
        labelized_x = pd.concat([pd.DataFrame(binned_x[:, continuous_features_indices], columns=continuous_features),
                                 df.iloc[:, -1]],
                                axis=&#39;columns&#39;)

    return labelized_x


def dataset_to_corels_file(df, output):
    &#34;&#34;&#34;Generate the binary output suitable for a CORELS run from
    the given `df` in file `output`.
    Args:
        df (DataFrame): The data to binarize.
        output (str): The output file prefix. The output files
                        are `output`.data.corels and `output`.label.corels
    &#34;&#34;&#34;
    binaryzed_df = _binary_discretization(df, name=output)
    columns = binaryzed_df.columns
    descriptions = columns.tolist()[:-1]
    descriptions = [&#39;{&#39; + desc.replace(&#39;=&#39;, &#39;:&#39;) + &#39;} &#39; for desc in descriptions]
    values = [binaryzed_df[c].astype(int).astype(str) for c in columns[:-1]]
    feature_strings = [&#39; &#39;.join(feature_values) + &#39;\n&#39; for feature_values in values]
    feature_strings = [description + value for description, value in zip(descriptions, feature_strings)]
    feature_strings = &#39;&#39;.join(feature_strings)
    y = binaryzed_df.astype(int).values[:, -1]
    not_y = ((y + 1) % 2).astype(int)
    class_strings = [&#39;{y:1} &#39; + &#39; &#39;.join(y.astype(str)),
                     &#39;{y:0} &#39; + &#39; &#39;.join(not_y.astype(str))]
    class_strings = &#39;\n&#39;.join(class_strings)

    # Dump on disk
    with open(output + &#39;.data.corels&#39;, &#39;w&#39;) as log:
        log.write(feature_strings)
    with open(output + &#39;.label.corels&#39;, &#39;w&#39;) as log:
        log.write(class_strings)

    return feature_strings, class_strings


def dataset_to_sbrl_file(df, output):
    &#34;&#34;&#34;Generate the binary output suitable for a SBRL run from
    the given `df` in file `output`.
    Args:
        df (DataFrame): The data to binarize.
        output (str): The output file prefix. The output files
                        are `output`.data.sbrl and `output`.label.sbrl
    &#34;&#34;&#34;
    binaryzed_df = _binary_discretization(df, name=output)
    columns = binaryzed_df.columns
    descriptions = columns.tolist()[:-1]
    descriptions = [&#39;{&#39; + desc.replace(&#39;=&#39;, &#39;:&#39;) + &#39;} &#39; for desc in descriptions]
    values = [binaryzed_df[c].astype(int).astype(str) for c in columns[:-1]]
    feature_strings = [&#39; &#39;.join(feature_values) + &#39;\n&#39; for feature_values in values]
    feature_strings = [description + value for description, value in zip(descriptions, feature_strings)]
    feature_strings = &#39;&#39;.join(feature_strings)
    y = binaryzed_df.astype(int).values[:, -1]
    not_y = ((y + 1) % 2).astype(int)
    class_strings = [&#39;{y:1} &#39; + &#39; &#39;.join(y.astype(str)),
                     &#39;{y:0} &#39; + &#39; &#39;.join(not_y.astype(str))]
    class_strings = &#39;\n&#39;.join(class_strings)
    header = &#39;n_items: &#39; + str(df.shape[1]) + &#39;\n&#39;
    header = header + &#39;n_samples: &#39; + str(df.shape[0]) + &#39;\n&#39;
    header_labels = &#39;n_items: 2\n&#39;
    header_labels = header_labels + &#39;n_samples: &#39; + str(df.shape[0]) + &#39;\n&#39;

    # Dump on disk
    with open(output + &#39;.data.sbrl&#39;, &#39;w&#39;) as log:
        log.write(header + feature_strings)
    with open(output + &#39;.label.sbrl&#39;, &#39;w&#39;) as log:
        log.write(header_labels + class_strings)


def _decision_set_discretization(df):
    &#34;&#34;&#34;Discretize the given dataset `df` to accomodate the
    Decision Set internal representation.
    Args:
        df (DataFrame): The dataset.
    Returns:
        (DataFrame): The discretized dataset.
    &#34;&#34;&#34;
    ids_df = pd.DataFrame(_label_discretization(df))

    return ids_df


def _decision_set_rule(decision_set_rule):
    &#34;&#34;&#34;Encode the given Decision Rule into a `Rule`.
    Args:
        decision_set_rule (rule): Rule in the Decision Set format.
    Returns:
        (Rule): Rule in the `Rule` format.
    &#34;&#34;&#34;
    return None


def _jrbc(alg, df):
    &#34;&#34;&#34;
    Build and train a Java Rule-Based Classifier adversary.

    Arguments:
        df (ndarray): The TR set.

    Returns:
        (RuleBasedClassifier): A Java Rule-Based Classifier trained on @alg algorithm.
    &#34;&#34;&#34;
    jrbc = RuleBasedClassifier(alg, options=&#39;-Xmx4G&#39;)
    df_, encoders, discretizer, scaling_factors = __encode_for_jrbc(df)
    x, y = df_.values[:, :-1], df_.values[:, -1]
    jrbc.fit(x, y, verbose=False)

    return jrbc, encoders, discretizer, scaling_factors


def _cpar(df, path, cwd):
    &#34;&#34;&#34;Create a CPAR model from @patterns.
    Arguments:
        df (DataFrame): A numeric-only ndarray holding the TR data.

    Returns:
        (CPAR): A CPAR classifier.
    &#34;&#34;&#34;
    os.chdir(path)
    cpar, encoders, discretizer, scaling_factors = _jrbc(&#39;CPAR&#39;, df)
    cpar = CPAR(cpar, encoders, discretizer, scaling_factors, df.dtypes, path, cwd)
    os.chdir(cwd)

    return cpar


def _foil(df, path, cwd):
    &#34;&#34;&#34;Create a CPAR model from @patterns.
    Arguments:
        patterns (ndarray): A numeric-only ndarray holding the TR data.

    Returns:
        (FOIL): A FOIL classifier.
    &#34;&#34;&#34;
    os.chdir(path)
    foil, encoders, discretizer, scaling_factors = _jrbc(&#39;FOIL&#39;, df)
    foil = FOIL(foil, encoders, discretizer, scaling_factors, df.dtypes, path, cwd)
    os.chdir(cwd)

    return foil


def _rules_from_cpar(cpar):
    &#34;&#34;&#34;Extract the rules generated by @cpar.

    Arguments:
        cpar (CPAR): A CPAR classifier.

    Returns:
        (set): A set of rules extracted from @cpar.
    &#34;&#34;&#34;
    cpar_rules = cpar.cpar.rules
    encoders = cpar.encoders
    scaling_factors = cpar.scaling_factors
    if len(cpar_rules) == 0:
        return []

    outcomes = array(list(map(lambda r: r[0], cpar_rules))) + 2
    outcomes -= sum(scaling_factors)
    ranges = list(map(lambda r: r[1], cpar_rules))
    ranges = list(filter(lambda r: len(r) &gt; 0, ranges))
    decoded_ranges = []
    for rule_ranges in ranges:
        ranges_dic = {}
        for feature in rule_ranges.keys():
            # Continuous
            if cpar.types[feature] == float64:
                bin = rule_ranges[feature] - sum(scaling_factors[:feature + 1])
                interval = cpar.discretizer.names[feature][bin]
                splits = interval.split(&#39;&lt;&#39;)
                # feature in [a, b]
                if len(splits) == 3:
                    ranges_dic[feature] = (float64(splits[0]), float64(splits[2][2:]))  # Skip first character, is a &#39;= &#39;
                # feature &lt; a,
                elif len(splits) == 2:
                    ranges_dic[feature] = (-inf, float64(splits[1][1:]))  # Skip first character, is an &#39;=&#39;
                # feature &gt; b
                else:
                    val = float64(interval.split(&#39;&gt; &#39;)[1])
                    ranges_dic[feature] = (val, +inf)
            # Integers
            elif cpar.types[feature] == int64:
                bin = rule_ranges[feature] - sum(scaling_factors[:feature + 1])
                interval = cpar.discretizer.names[feature][bin]
                splits = interval.split(&#39;&lt;&#39;)
                # feature in [a, b]
                if len(splits) == 3:
                    ranges_dic[feature] = (int(rint(float64(splits[0]))),
                                           int(rint(float64(splits[2][2:]))))  # Skip first two characters, is a &#39;= &#39;
                # feature &lt; a,
                elif len(splits) == 2:
                    ranges_dic[feature] = (-inf, int(rint(float64(splits[1][1:]))))  # Skip first character, is an &#39;=&#39;
                # feature &gt; b
                else:
                    val = rint(int(float64(interval.split(&#39;&gt; &#39;)[1])))
                    ranges_dic[feature] = (val, +inf)
            # Categorical
            else:
                val = encoders[feature].inverse_transform(rule_ranges[feature] - sum(scaling_factors[:feature + 1]))
                ranges_dic[feature] = (val, val)

        decoded_ranges.append(ranges_dic)

    cpar_rules = [Rule(premises={}, consequence=outcome) for outcome in outcomes]
    for rule, rule_range in zip(cpar_rules, decoded_ranges):
        rule.premises = rule_range

    return cpar_rules


def _rules_from_foil(foil):
    &#34;&#34;&#34;Extract the rules generated by @cpar.

    Arguments:
        foil (FOIL): A FOIL classifier.
        encoders (list): Encoders to go from numerical to original
                            encoding. If None, the numerical encoding
                            is returned.
    Returns:
        {set): A set of rules extracted from @cpar.
    &#34;&#34;&#34;
    encoders = foil.encoders
    scaling_factors = foil.scaling_factors
    foil_rules = foil.foil.rules
    if len(foil_rules) == 0:
        return []

    outcomes = array(list(map(lambda r: r[0], foil_rules))) + 2
    outcomes -= sum(scaling_factors)
    ranges = list(map(lambda r: r[1], foil_rules))
    ranges = list(filter(lambda r: len(r) &gt; 0, ranges))
    decoded_ranges = []
    for rule_ranges in ranges:
        ranges_dic = {}
        for feature in rule_ranges.keys():
            # Continuous
            if foil.types[feature] == float64:
                bin = rule_ranges[feature] - sum(scaling_factors[:feature + 1])
                interval = foil.discretizer.names[feature][bin]
                splits = interval.split(&#39;&lt;&#39;)
                # feature in [a, b]
                if len(splits) == 3:
                    ranges_dic[feature] = (float64(splits[0]), float64(splits[2][2:]))  # Skip first character, is a &#39;= &#39;
                # feature &lt; a,
                elif len(splits) == 2:
                    ranges_dic[feature] = (-inf, float64(splits[1][1:]))  # Skip first character, is an &#39;=&#39;
                # feature &gt; b
                else:
                    val = float64(interval.split(&#39;&gt; &#39;)[1])
                    ranges_dic[feature] = (val, +inf)
            # Integers
            elif foil.types[feature] == int64:
                bin = rule_ranges[feature] - sum(scaling_factors[:feature + 1])
                interval = foil.discretizer.names[feature][bin]
                splits = interval.split(&#39;&lt;&#39;)
                # feature in [a, b]
                if len(splits) == 3:
                    ranges_dic[feature] = (int(rint(float64(splits[0]))),
                                           int(rint(float64(splits[2][2:]))))  # Skip first two characters, is a &#39;= &#39;
                # feature &lt; a,
                elif len(splits) == 2:
                    ranges_dic[feature] = (-inf, int(rint(float64(splits[1][1:]))))  # Skip first character, is an &#39;=&#39;
                # feature &gt; b
                else:
                    val = rint(int(float64(interval.split(&#39;&gt; &#39;)[1])))
                    ranges_dic[feature] = (val, +inf)
            # Categorical
            else:
                val = encoders[feature].inverse_transform(rule_ranges[feature] - sum(scaling_factors[:feature + 1]))
                ranges_dic[feature] = (val, val)

        decoded_ranges.append(ranges_dic)

    foil_rules = [Rule(premises={}, consequence=outcome) for outcome in outcomes]
    for rule, rule_range in zip(foil_rules, decoded_ranges):
        rule.premises = rule_range

    return foil_rules


def _rule_from_anchor(anchor):
    &#34;&#34;&#34;Extract a rule from an ANCHOR.
    Args:
        (AnchorExplanation): The ANCHOR explanation.
    Returns:
        (Rule): An explanation.
    &#34;&#34;&#34;
    tokens = list(map(lambda name: name.split(&#39; &#39;), anchor.names()))
    premises = {}

    for token in tokens:
        if len(token) == 3:
            if token[1] == &#39;&lt;=&#39; or token[1] == &#39;&lt;&#39;:
                premises[int(token[0])] = (-inf, float(token[2]))
            elif token[1] == &#39;&gt;=&#39; or token[1] == &#39;&gt;&#39;:
                premises[int(token[0])] = (float(token[2]), +inf)
        else:
            premises[int(token[2])] = (float(token[0]), float(token[4]))

    rule = Rule(premises=premises, consequence=int(anchor.exp_map[&#39;prediction&#39;]))

    return rule


def _rule_from_lore(explanation):
    &#34;&#34;&#34;Extract a rule from an ANCHOR.
    Args:
        (Explanation): The LORE explanation.
    Returns:
        (Rule): An explanation.
    &#34;&#34;&#34;
    consequence = int(explanation.rule.cons)
    premises = explanation.rule.premises
    features = [int(condition.att) for condition in premises]
    ops = [premise.op for premise in premises]
    values = [float(premise.thr) for premise in premises]
    values_per_feature = {feature: [val for f, val in zip(features, values) if int(f) == feature]
                          for feature in features}
    ops_per_feature = {feature: [op for f, op in zip(features, ops) if f == feature]
                       for feature in features}
    output_premises = {}
    for f in features:
        values, operators = values_per_feature[f], ops_per_feature[f]
        # 1 value, either &lt;= or &gt;
        if len(values) == 1:
            if operators[0] == &#39;&lt;=&#39;:
                output_premises[f] = (-inf, values[0])
            else:
                output_premises[f] = (values[0], +inf)
        # 2 values, &lt; x &lt;=
        else:
            output_premises[f] = (min(values), max(values))

    rule = Rule(premises=output_premises, consequence=consequence)

    return rule


def _rule_from_trepan(trepan):
    &#34;&#34;&#34;Convert the provided Trepan `rule` to a `Rule`.
    Args:
        trepan (TREPAN): A Trepan instance.
    Returns:
        (list): A list of `Rule` object.
    &#34;&#34;&#34;
    rules = trepan_to_rules(trepan)
    rules = [Rule(rule.premises, rule.consequence) for rule in rules]

    return rules


def __children(v, edges):
    return list(map(lambda edge: int(edge[1][1:]),
                    list(filter(lambda edge: edge[0] == v, list(edges)))))


def __all_paths(tree):
    &#34;&#34;&#34;
    Retrieve all the possible paths in @tree.

    Arguments:
        tree {): The decision tree internals.

    Returns:
        (list): A list of list of indices:[path_1, path_2, .., path_m]
                    where path_i = [node_1, node_l].
    &#34;&#34;&#34;
    paths = [[0]]
    l_child = tree.children_left[0]
    r_child = tree.children_right[0]

    if tree.capacity == 1:
        return paths

    paths = paths + \
            __rec_all_paths(tree, r_child, [0], +1) + \
            __rec_all_paths(tree, l_child, [0], -1)
    paths = sorted(set(map(tuple, paths)), key=lambda p: len(p))

    return paths


def __rec_all_paths_yadt(tree, node, current_path):
    &#34;&#34;&#34;
    Recursive call for the @all_paths function.

    Arguments:
        tree (): The decision tree internals.
        node (int): The node whose path to expand.
        current_path (list): The current path root-&gt; @node.

    Returns:
        (list): The enriched path.
    &#34;&#34;&#34;
    if tree.node[&#39;n&#39; + str(node)][&#39;label&#39;].count(&#39;/&#39;) &gt; 0:
        return current_path + [node]
    else:
        node_children = __children(&#39;n&#39; + str(node), list(tree.edges))

        res = []
        for child in node_children:
            children_paths = __rec_all_paths_yadt(tree, child, current_path + [node])
            res.append(children_paths)

        return res


def __rec_all_paths(tree, node, path, direction):
    &#34;&#34;&#34;
    Recursive call for the @all_paths function.

    Arguments:
        tree (): The decision tree internals.
        node (int): The node whose path to expand.
        path (list): The path root-&gt; @node.
        direction (int):  +1 for right child, -1 for left child.
                            Used to store the actual traversal.

    Returns:
        (list): The enriched path.
    &#34;&#34;&#34;
    # Leaf
    if tree.children_left[node] == tree.children_right[node]:
        return [path + [node * direction]]
    else:
        path_ = [path + [node * direction]]
        l_child = tree.children_left[node]
        r_child = tree.children_right[node]

        return path_ + \
               __rec_all_paths(tree, r_child, path_[0], +1) + \
               __rec_all_paths(tree, l_child, path_[0], -1)


def _rules_from_dt(decision_tree):
    &#34;&#34;&#34;Extract the rules   applied by @dt for the provided
    @patterns, i.e. the path followed by each path when
    classified by @dt.

    Arguments:
        decision_tree (DecisionTreeClassifier): The decision tree whose rules to extract.

    Returns:
        (set): A set of rules extracted from `decision_tree`.
    &#34;&#34;&#34;
    tree = decision_tree.tree_

    paths = __all_paths(tree)
    paths = list(filter(lambda path: len(path) &gt; 1, paths))
    consequences = [argmax(tree.value[abs(path[-1])]) for path in paths]
    features = [list(map(lambda node: tree.feature[abs(node)], path[:-1])) for path in paths]
    thresholds = [list(map(lambda node: tree.threshold[abs(node)], path[:-1])) for path in paths]
    rules = [Rule.fromarrays(feature_list, thresholds_list, consequence_list, paths_list)
             for feature_list, thresholds_list, consequence_list, paths_list
             in zip(features, thresholds, consequences, paths)]

    return rules


def _rules_from_decision_set(decision_set):
    &#34;&#34;&#34;
    Extract rules from the given `decision_set`.
    Args:
        decision_set (DecisionSet): The Decision Set instance.
    Returns:
        (list): List of `Rule`.
    &#34;&#34;&#34;
    rules = decision_set.rules
    # TODO: Implement
    raise ValueError(&#39;To implement&#39;)

    return rules


class CPAR:
    &#34;&#34;&#34;CPAR classifier.&#34;&#34;&#34;

    def __init__(self, cpar, encoders, discretizer, scaling_factors, types, path, cwd):
        &#34;&#34;&#34;Constructor.
        Arguments:
            cpar: CPAR instance.
            encoders (list): Encoders.
            discretizer (BaseDiscretizer): Discretizer to scale data for CPAR.
            scaling_factors (iterable): Scaling factors to scale data for CPAR.
            types (iterable): Data types.
            path (str): Working directory.
            cwd (str): Current working directory.
        &#34;&#34;&#34;
        self.cpar = cpar
        self.encoders = encoders
        self.discretizer = discretizer
        self.scaling_factors = scaling_factors
        self.types = types
        self.path = path
        self.cwd = cwd

    def predict(self, x):
        &#34;&#34;&#34;Predict x.
        Arguments:
            x (ndarray): Data to predict.
        Returns:
            (ndarray): Predictions on x.
        &#34;&#34;&#34;
        x_ = x.copy()
        os.chdir(self.path)
        x_ = _decode_from_jrbc(DataFrame(x_), self.encoders, self.discretizer, self.scaling_factors)
        y = self.cpar.predict(x_)
        scaled_y = y - self.scale[-1]
        os.chdir(self.cwd)

        # Fix for missing values in the TR
        scaled_y -= min(scaled_y)

        return scaled_y


class FOIL:
    &#34;&#34;&#34;FOIL classifier.&#34;&#34;&#34;

    def __init__(self, foil, encoders, discretizer, scaling_factors, types, path, cwd):
        &#34;&#34;&#34;Constructor.
        Arguments:
            cpar: FOIL instance.
            encoders (list): Encoders.
            discretizer (BaseDiscretizer): Discretizer to scale data for FOIL.
            scaling_factors (iterable): Scaling factors to scale data for FOIL.
            types (iterable): Data types.
            path (str): Working directory.
            cwd (str): Current working directory.
        &#34;&#34;&#34;
        self.foil = foil
        self.encoders = encoders
        self.discretizer = discretizer
        self.scaling_factors = scaling_factors
        self.types = types
        self.path = path
        self.cwd = cwd

    def predict(self, x):
        &#34;&#34;&#34;Predict x.
        Arguments:
            x (ndarray): Data to predict.
        Returns:
            (ndarray): Predictions on x.
        &#34;&#34;&#34;
        x_ = x.copy()
        os.chdir(self.path)

        for i, scale in enumerate(self.scale[:-1]):
            x_[:, i] += scale

        y = self.foil.predict(x_)
        scaled_y = y - self.scale[-1]
        os.chdir(self.cwd)

        # Fix for missing values in the TR
        scaled_y -= min(scaled_y)

        return scaled_y


class DecisionSet:
    &#34;&#34;&#34;Decision set instance.&#34;&#34;&#34;

    def __init__(self):
        self.rules = None

    def fit(self, x, y):
        &#34;&#34;&#34;Train this instance.
        Args:
            x (ndarray): Data.
            y (ndarray): Labels.
        Returns:
            (DecisionSet): Returns `self`.
        &#34;&#34;&#34;
        return None


def decision_tree(x, y):
    &#34;&#34;&#34;Train a Decision Tree Classifier on x and y.
    Arguments:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of `Rule`.
    &#34;&#34;&#34;
    dt = DecisionTreeClassifier()
    dt.fit(x, y)
    rules = _rules_from_dt(dt)

    return rules


def pruned_decision_tree(x, y, max_depth=4):
    &#34;&#34;&#34;Train a Decision Tree Classifier on `x` and `y` with maximum depth `max_depth`.
    Arguments:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of `Rule`.
    &#34;&#34;&#34;
    dt = DecisionTreeClassifier(max_depth=max_depth)
    dt.fit(x, y)
    rules = _rules_from_dt(dt)

    return rules


def cpar(x, y, path=None, cwd=None):
    &#34;&#34;&#34;Train a CPAR on data x and labels y.
    Arguments:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): The CPAR rules.
    &#34;&#34;&#34;
    if path is None:
        path = os.getcwd()
    if cwd is None:
        cwd = os.getcwd()

    df = DataFrame(hstack((x, y.reshape(-1, 1)))).convert_objects(convert_numeric=True)
    model = _cpar(df, path, cwd)
    rules = _rules_from_cpar(model)

    return rules


def foil(x, y, path=None, cwd=None):
    &#34;&#34;&#34;Train a FOIL on data x and labels y.
    Arguments:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): The FOIL rules.
    &#34;&#34;&#34;
    if path is None:
        path = os.getcwd()
    if cwd is None:
        cwd = os.getcwd()

    df = DataFrame(hstack((x, y.reshape(-1, 1)))).convert_objects(convert_numeric=True)
    model = _foil(df, path, cwd)
    rules = _rules_from_foil(model)

    return rules


def anchors(x, y, oracle):
    &#34;&#34;&#34;ANCHORS adversary.
    Args:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of Anchors rules.
    &#34;&#34;&#34;
    # List of class values to map to integers
    class_names = [0, 1]
    # List of features, class excluded
    feature_names = range(x.shape[1])
    # binary
    values_per_feature = [sorted(set(x[:, k])) for k in feature_names]
    binary_features = [k for k in feature_names if values_per_feature[k] == [0, 1]]
    categorical_names = {k: values_per_feature[k] for k in binary_features}
    categorical_names = {}

    # Constructor with class names, feature names and development data
    explainer = AnchorTabularExplainer(class_names, feature_names, x, categorical_names)
    explainer.fit(x, y, x, y)

    # Extract anchors
    rules = []
    for i in tqdm(range(x.shape[0])):
        explanation = explainer.explain_instance(x[i].reshape(1, -1), oracle.predict, threshold=0.95)
        rule = _rule_from_anchor(explanation)
        rules.append(rule)

    return rules


def lore(x, y, oracle):
    &#34;&#34;&#34;LORE adversary.
    Args:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of LORE rules.
    &#34;&#34;&#34;
    # List of class values to map to integers
    # List of features, class excluded
    df = pd.DataFrame((hstack((x, y.reshape(-1, 1)))))
    df.columns = list(map(str, range(df.shape[1])))
    class_name = df.columns[-1]

    df, feature_names, class_values, numeric_columns, rdf, real_feature_names, features_map = prepare_dataset(df, class_name)
    # Constructor with class names, feature names and development data
    explainer = LOREM(x, oracle.predict, feature_names, class_name, class_values, numeric_columns,
                      features_map, neigh_type=&#39;rndgen&#39;, categorical_use_prob=True,
                      continuous_fun_estimation=False, size=750, ocr=0.1, multi_label=False, one_vs_rest=False,
                      verbose=False, ngen=10)

    # Extract anchors
    rules = []
    for i in tqdm(range(x.shape[0])):
        explanation = explainer.explain_instance(x[i], use_weights=True, metric=&#39;euclidean&#39;)
        rule = _rule_from_lore(explanation)
        rules.append(rule)

    return rules


def trepan(x, y, oracle):
    &#34;&#34;&#34;Train a Trepan instance.
    Args:
        x (ndarray): Data.
        y (ndarray): Labels.
        oracle (Predictor): The oracle used to grow the train data.
    Returns:
        (list): List of rules.
    &#34;&#34;&#34;
    trepan = TREPAN(oracle)
    data = pd.DataFrame(hstack((x, y.reshape(-1, 1))))
    trepan = trepan.fit(data, max_nodes=3)
    rules = _rule_from_trepan(trepan)

    return rules


def decision_set(x, y):
    &#34;&#34;&#34;Train a Decision Set instance.
    Args:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of rules.
    &#34;&#34;&#34;
    decision_set  = DecisionSet()
    decision_set = decision_set.fit(x, y)
    rules = _rules_from_decision_set(decision_set)

    return rules


def corels(data, labels, undiscretize, one_hot_names):
    &#34;&#34;&#34;BRL adversary.
    Args:
        data (str): Training data path.
        label (str): Training labels path.
        undiscretize (dict): Dictionary feature -&gt; bins.
        one_hot_names (list): List of one-hot names.
    Returns:
        (list): List of CORELS rules.
    &#34;&#34;&#34;
    cmd = wd + &#39;corels/src/corels -r 0.015 -c 2 -p 1 &#34;&#39; + data + &#39;&#34; &#34;&#39; + labels + &#39;&#34;&#39;
    process = run_cmd(cmd, shell=True, stdout=PIPE)
    output = process.stdout.decode(&#39;utf-8&#39;).split(&#39;\n&#39;)
    rules_start = output.index(&#39;OPTIMAL RULE LIST&#39;) + 1
    rules_end = rules_start + output[rules_start:].index(&#39;&#39;)
    corels_rules = [output[k] for k in range(rules_start, rules_end)]

    with open(data, &#39;r&#39;) as log:
        data_file = log.read()
    features = re.findall(&#39;\{.+:.+\} &#39;, data_file)
    feature_premises = [f[1:-2].split(&#39;:&#39;) for f in features]
    groups = [0] + [i for i in range(len(feature_premises) - 1)
                    if feature_premises[i][0] != feature_premises[i + 1][0]] + [len(feature_premises) - 1]
    categorical_feature_values = {}
    for g, group in enumerate(groups):
        if g &lt; len(groups) - 2 and not feature_premises[groups[g + 1] + 1][1].isdigit():
            feature_name = feature_premises[groups[g] + 1][0]
            feature_cardinality = groups[g + 1] - groups[g]
            categorical_feature_values[feature_name] = [feature_premises[groups[g] + 1 + k][1] for k in
                                                        range(feature_cardinality)]
        elif g == len(groups) - 2 and not feature_premises[groups[-2] + 1][1].isdigit():
            feature_name = feature_premises[groups[-1]][0]
            feature_cardinality = groups[-1] - groups[-2]
            categorical_feature_values[feature_name] = [feature_premises[groups[g] + 1 + k][1] for k in
                                                        range(feature_cardinality)]

    rules = []
    for corels_rule in corels_rules:
        parsed_corels_rule = corels_rule.replace(&#39;{y:&#39;, &#39;&#39;).replace(&#39;{&#39;, &#39;&#39;).replace(&#39;}&#39;, &#39;&#39;)\
                            .replace(&#39;(&#39;, &#39;&#39;).replace(&#39;)&#39;, &#39;&#39;).replace(&#39;if &#39;, &#39;&#39;).replace(&#39; then&#39;, &#39;&#39;).replace(&#39;else &#39;, &#39;&#39;)
        corels_premises = parsed_corels_rule.split(&#39; &#39;)
        output = int(corels_premises[-1])
        corels_premises = corels_premises[:-1]

        premises = {}
        # Last one is the default empty rule
        for corels_premise in corels_premises:
            corels_premise = corels_premise.replace(&#39;{&#39;, &#39;&#39;).replace(&#39;}&#39;, &#39;&#39;)

            # One-hot with positive value
            if &#39;:&#39; not in corels_premise:
                premises[int(corels_premise)] = (.5, +inf)
                continue

            feature, value = corels_premise.split(&#39;:&#39;)
            if value.isdigit() and int(value) in one_hot_names:
                feature, value = int(feature), int(value)
                idx = one_hot_names.index(feature if feature.isdigit() else feature)
                premises[idx] = tuple(undiscretize[feature][value])
            elif not value.isdigit() and feature + &#39;=&#39; + value in one_hot_names:
                # One-hot encoding
                idx = one_hot_names.index(feature + &#39;=&#39; + value)
                premises[idx] = 0.5, +inf
            else:
                idx = one_hot_names[int(feature)]
                premises[int(feature)] = tuple(undiscretize[idx][int(value)])

        rule = Rule(premises=premises, consequence=output)
        rules.append(rule)

    return rules


def bayesian_rule_lists(data, labels, undiscretize, one_hot_names):
    &#34;&#34;&#34;SBRL adversary.
    Args:
        data (str): Training data path.
        label (str): Training labels path.
        undiscretize (dict): Dictionary feature -&gt; bins.
        one_hot_names (list): List of one-hot names.
    Returns:
        (list): List of SBRL rules.
    &#34;&#34;&#34;
    rule_ids, outputs, rule_strings = pysbrl.train_sbrl(data, labels, 20.0, eta=2.0, max_iters=2000, n_chains=10)

    outputs = outputs.argmax(axis=1)
    sbrl_rules = [rule_strings[i] for i in rule_ids]
    sbrl_rules = [rule for rule in sbrl_rules if rule != &#39;default&#39;]
    rules = []

    with open(data, &#39;r&#39;) as log:
        data_file = log.read()
    features = re.findall(&#39;\{.+:.+\} &#39;, data_file)
    feature_premises = [f[1:-2].split(&#39;:&#39;) for f in features]
    groups = [0] + [i for i in range(len(feature_premises) - 1)
                    if feature_premises[i][0] != feature_premises[i + 1][0]] + [len(feature_premises) - 1]

    categorical_feature_values = {}
    for g, group in enumerate(groups):
        if g &lt; len(groups) - 2 and not feature_premises[groups[g + 1] + 1][1].isdigit():
            feature_name = feature_premises[groups[g] + 1][0]
            feature_cardinality = groups[g + 1] - groups[g]
            categorical_feature_values[feature_name] = [feature_premises[groups[g] + 1 + k][1] for k in range(feature_cardinality)]
        elif g == len(groups) - 2 and not feature_premises[groups[-2] + 1][1].isdigit():
            feature_name = feature_premises[groups[-1]][0]
            feature_cardinality = groups[-1] - groups[-2]
            categorical_feature_values[feature_name] = [feature_premises[groups[g] + 1 + k][1] for k in
                                                 range(feature_cardinality)]

    for rule, output in zip(sbrl_rules[:-1], outputs):
        sbrl_premises = rule.replace(&#39;{&#39;, &#39;&#39;).replace(&#39;}&#39;, &#39;&#39;).split(&#39;,&#39;)
        premises = {}
        # Last one is the default empty rule
        for sbrl_premise in sbrl_premises:
            sbrl_premise = sbrl_premise.replace(&#39;{&#39;, &#39;&#39;).replace(&#39;}&#39;, &#39;&#39;)

            # One-hot with positive value
            if &#39;:&#39; not in sbrl_premise:
                premises[int(sbrl_premise)] = (.5, +inf)
                continue

            feature, value = sbrl_premise.split(&#39;:&#39;)
            if feature.isdigit() and value is not None:
                feature, value = int(feature), int(value)
                try:
                    premises[feature] = tuple(undiscretize[one_hot_names[feature]][value])
                except KeyError:
                    pass
            elif value is None:
                # One-hot encoding
                feature = int(feature)
                premises[feature] = 0.5, +inf
            else:
                idx = one_hot_names[int(feature)]
                premises[int(feature)] = tuple(undiscretize[idx][int(value)])

        rule = Rule(premises=premises, consequence=output)
        rules.append(rule)

    return rules


@click.command()
@click.option(&#39;-tr&#39;,     type=click.Path(exists=True),   help=&#39;Path to the training set.&#39;)
@click.option(&#39;-ts&#39;,     type=click.Path(exists=True),   help=&#39;Path to the test set.&#39;)
@click.option(&#39;-m&#39;,     &#39;--model&#39;,      default=None,       help=&#39;Model to train. Can be either \&#39;decision tree\&#39;,&#39;
                                                      &#39; \&#39;pruned_decision_tree\&#39;, \&#39;cpar\&#39;, \&#39;foil\&#39;,&#39;
                                                       &#39; \&#39;trepan\&#39;, \&#39;ids\&#39;, \&#39;anchors\&#39;.&#39;)
@click.option(&#39;--dataset&#39;,    default=None,       help=&#39;Dataset name. Used to store results.&#39;)
@click.option(&#39;-d&#39;,     &#39;--data&#39;,       default=None,       help=&#39;Data path. Provide when using either CORELS or SBRL.&#39;,
                type=click.Path(exists=True))
@click.option(&#39;-l&#39;,     &#39;--labels&#39;,     default=None,       help=&#39;Labels path. Provide when using either CORELS or SBRL.&#39;,
                type=click.Path(exists=True))
@click.option(&#39;-b&#39;,     &#39;--buckets&#39;,    default=None,       help=&#39;Buckets path. Provide when using either CORELS or SBRL.&#39;,
                type=click.Path(exists=True))
@click.option(&#39;-n&#39;,     &#39;--names&#39;,      default=None,       help=&#39;One-hot names. Provide when using either CORELS or SBRL.&#39;,
                type=click.Path(exists=True))
@click.option(&#39;-o&#39;,     &#39;--oracle&#39;,     default=None,       help=&#39;Oracle file, if required by the model.&#39;,
              type=click.Path(exists=True))
@click.option(&#39;-p&#39;,     &#39;--path&#39;,       default=None,       help=&#39;Output path to store the generated ruleset.&#39;)
def cl_run(tr, ts, model, dataset, data, labels, buckets, names, oracle, path):
    if model in (&#39;anchors&#39;, &#39;trepan&#39;, &#39;lore&#39;) and oracle is None:
        raise ValueError(&#39;No oracle provided.&#39;)

    if oracle is not None:
        if oracle.endswith(&#39;.h5&#39;):
            black_box = load_model(oracle)
        elif oracle.endswith(&#39;.pickle&#39;):
            with open(oracle, &#39;rb&#39;) as log:
                black_box = pickle.load(log)

    if names is not None:
        one_hot_names = pd.read_csv(names).values.tolist()[0]

    if buckets is not None:
        with open(buckets, &#39;r&#39;) as log:
            discretization = json.load(log)

    if tr is not None and model != &#39;corels&#39; and model != &#39;sbrl&#39;:
        tr = pd.read_csv(tr, delimiter=&#39;,&#39;).convert_objects(convert_numeric=True)
        x, y = tr.values[:, :-1], tr.values[:, -1]

        if oracle is not None:
            y = black_box.predict(x).squeeze()

    if model == &#39;lore&#39;:
        rules = lore(x, y, black_box)
    elif model == &#39;anchors&#39;:
        rules = anchors(x, y, black_box)
    elif model == &#39;cpar&#39;:
        rules = cpar(x, y)
    elif model == &#39;foil&#39;:
        rules = foil(x, y)
    elif model == &#39;decision_tree&#39;:
        rules = decision_tree(x, y)
    elif model == &#39;pruned_decision_tree&#39;:
        rules = pruned_decision_tree(x, y)
    elif model == &#39;trepan&#39;:
        rules = trepan(x, y, black_box)
    elif model == &#39;sbrl&#39;:
        if data is None:
            tr = pd.read_csv(tr, delimiter=&#39;,&#39;).convert_objects(convert_numeric=True)
            ts = pd.read_csv(ts, delimiter=&#39;,&#39;).convert_objects(convert_numeric=True)
            full_data = pd.concat([tr, ts], axis=&#39;rows&#39;, ignore_index=True).dropna()

            if oracle is not None:
                y = black_box.predict(full_data.values[:, :-1]).round().squeeze()
                full_data.iloc[:, -1] = y
            output = &#39;.&#39; + str(datetime.datetime.now())

            dataset_to_sbrl_file(full_data, output)
            data = output + &#39;.data.sbrl&#39;
            labels = output + &#39;.label.sbrl&#39;

        rules = bayesian_rule_lists(data, labels, undiscretize=discretization, one_hot_names=one_hot_names)
    elif model == &#39;corels&#39;:
        if data is None:
            tr = pd.read_csv(tr, delimiter=&#39;,&#39;).convert_objects(convert_numeric=True)
            ts = pd.read_csv(ts, delimiter=&#39;,&#39;).convert_objects(convert_numeric=True)
            full_data = pd.concat([tr, ts], axis=&#39;rows&#39;, ignore_index=True)

            if oracle is not None:
                y = black_box.predict(full_data.values[:, :-1]).round().squeeze()
                full_data.iloc[:, -1] = y
            output = &#39;.&#39; + str(datetime.datetime.now())

            dataset_to_corels_file(full_data, output)
            data = output + &#39;.data.corels&#39;
            labels = output + &#39;.label.corels&#39;

        rules = corels(data, labels, undiscretize=discretization, one_hot_names=one_hot_names)
    else:
        raise ValueError(&#39;Unknown model: &#39; + str(model))

    with open(path, &#39;w&#39;) as log:
        print(&#39;Dumping in &#39; + path)
        json_rules = [rule.json() for rule in rules]
        json.dump(json_rules, log)


if __name__ == &#39;__main__&#39;:
    cl_run()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="adversaries.anchors"><code class="name flex">
<span>def <span class="ident">anchors</span></span>(<span>x, y, oracle)</span>
</code></dt>
<dd>
<section class="desc"><p>ANCHORS adversary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): List of Anchors rules.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def anchors(x, y, oracle):
    &#34;&#34;&#34;ANCHORS adversary.
    Args:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of Anchors rules.
    &#34;&#34;&#34;
    # List of class values to map to integers
    class_names = [0, 1]
    # List of features, class excluded
    feature_names = range(x.shape[1])
    # binary
    values_per_feature = [sorted(set(x[:, k])) for k in feature_names]
    binary_features = [k for k in feature_names if values_per_feature[k] == [0, 1]]
    categorical_names = {k: values_per_feature[k] for k in binary_features}
    categorical_names = {}

    # Constructor with class names, feature names and development data
    explainer = AnchorTabularExplainer(class_names, feature_names, x, categorical_names)
    explainer.fit(x, y, x, y)

    # Extract anchors
    rules = []
    for i in tqdm(range(x.shape[0])):
        explanation = explainer.explain_instance(x[i].reshape(1, -1), oracle.predict, threshold=0.95)
        rule = _rule_from_anchor(explanation)
        rules.append(rule)

    return rules</code></pre>
</details>
</dd>
<dt id="adversaries.bayesian_rule_lists"><code class="name flex">
<span>def <span class="ident">bayesian_rule_lists</span></span>(<span>data, labels, undiscretize, one_hot_names)</span>
</code></dt>
<dd>
<section class="desc"><p>SBRL adversary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>str</code></dt>
<dd>Training data path.</dd>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code></dt>
<dd>Training labels path.</dd>
<dt><strong><code>undiscretize</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary feature -&gt; bins.</dd>
<dt><strong><code>one_hot_names</code></strong> :&ensp;<code>list</code></dt>
<dd>List of one-hot names.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): List of SBRL rules.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def bayesian_rule_lists(data, labels, undiscretize, one_hot_names):
    &#34;&#34;&#34;SBRL adversary.
    Args:
        data (str): Training data path.
        label (str): Training labels path.
        undiscretize (dict): Dictionary feature -&gt; bins.
        one_hot_names (list): List of one-hot names.
    Returns:
        (list): List of SBRL rules.
    &#34;&#34;&#34;
    rule_ids, outputs, rule_strings = pysbrl.train_sbrl(data, labels, 20.0, eta=2.0, max_iters=2000, n_chains=10)

    outputs = outputs.argmax(axis=1)
    sbrl_rules = [rule_strings[i] for i in rule_ids]
    sbrl_rules = [rule for rule in sbrl_rules if rule != &#39;default&#39;]
    rules = []

    with open(data, &#39;r&#39;) as log:
        data_file = log.read()
    features = re.findall(&#39;\{.+:.+\} &#39;, data_file)
    feature_premises = [f[1:-2].split(&#39;:&#39;) for f in features]
    groups = [0] + [i for i in range(len(feature_premises) - 1)
                    if feature_premises[i][0] != feature_premises[i + 1][0]] + [len(feature_premises) - 1]

    categorical_feature_values = {}
    for g, group in enumerate(groups):
        if g &lt; len(groups) - 2 and not feature_premises[groups[g + 1] + 1][1].isdigit():
            feature_name = feature_premises[groups[g] + 1][0]
            feature_cardinality = groups[g + 1] - groups[g]
            categorical_feature_values[feature_name] = [feature_premises[groups[g] + 1 + k][1] for k in range(feature_cardinality)]
        elif g == len(groups) - 2 and not feature_premises[groups[-2] + 1][1].isdigit():
            feature_name = feature_premises[groups[-1]][0]
            feature_cardinality = groups[-1] - groups[-2]
            categorical_feature_values[feature_name] = [feature_premises[groups[g] + 1 + k][1] for k in
                                                 range(feature_cardinality)]

    for rule, output in zip(sbrl_rules[:-1], outputs):
        sbrl_premises = rule.replace(&#39;{&#39;, &#39;&#39;).replace(&#39;}&#39;, &#39;&#39;).split(&#39;,&#39;)
        premises = {}
        # Last one is the default empty rule
        for sbrl_premise in sbrl_premises:
            sbrl_premise = sbrl_premise.replace(&#39;{&#39;, &#39;&#39;).replace(&#39;}&#39;, &#39;&#39;)

            # One-hot with positive value
            if &#39;:&#39; not in sbrl_premise:
                premises[int(sbrl_premise)] = (.5, +inf)
                continue

            feature, value = sbrl_premise.split(&#39;:&#39;)
            if feature.isdigit() and value is not None:
                feature, value = int(feature), int(value)
                try:
                    premises[feature] = tuple(undiscretize[one_hot_names[feature]][value])
                except KeyError:
                    pass
            elif value is None:
                # One-hot encoding
                feature = int(feature)
                premises[feature] = 0.5, +inf
            else:
                idx = one_hot_names[int(feature)]
                premises[int(feature)] = tuple(undiscretize[idx][int(value)])

        rule = Rule(premises=premises, consequence=output)
        rules.append(rule)

    return rules</code></pre>
</details>
</dd>
<dt id="adversaries.corels"><code class="name flex">
<span>def <span class="ident">corels</span></span>(<span>data, labels, undiscretize, one_hot_names)</span>
</code></dt>
<dd>
<section class="desc"><p>BRL adversary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>str</code></dt>
<dd>Training data path.</dd>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code></dt>
<dd>Training labels path.</dd>
<dt><strong><code>undiscretize</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary feature -&gt; bins.</dd>
<dt><strong><code>one_hot_names</code></strong> :&ensp;<code>list</code></dt>
<dd>List of one-hot names.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): List of CORELS rules.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def corels(data, labels, undiscretize, one_hot_names):
    &#34;&#34;&#34;BRL adversary.
    Args:
        data (str): Training data path.
        label (str): Training labels path.
        undiscretize (dict): Dictionary feature -&gt; bins.
        one_hot_names (list): List of one-hot names.
    Returns:
        (list): List of CORELS rules.
    &#34;&#34;&#34;
    cmd = wd + &#39;corels/src/corels -r 0.015 -c 2 -p 1 &#34;&#39; + data + &#39;&#34; &#34;&#39; + labels + &#39;&#34;&#39;
    process = run_cmd(cmd, shell=True, stdout=PIPE)
    output = process.stdout.decode(&#39;utf-8&#39;).split(&#39;\n&#39;)
    rules_start = output.index(&#39;OPTIMAL RULE LIST&#39;) + 1
    rules_end = rules_start + output[rules_start:].index(&#39;&#39;)
    corels_rules = [output[k] for k in range(rules_start, rules_end)]

    with open(data, &#39;r&#39;) as log:
        data_file = log.read()
    features = re.findall(&#39;\{.+:.+\} &#39;, data_file)
    feature_premises = [f[1:-2].split(&#39;:&#39;) for f in features]
    groups = [0] + [i for i in range(len(feature_premises) - 1)
                    if feature_premises[i][0] != feature_premises[i + 1][0]] + [len(feature_premises) - 1]
    categorical_feature_values = {}
    for g, group in enumerate(groups):
        if g &lt; len(groups) - 2 and not feature_premises[groups[g + 1] + 1][1].isdigit():
            feature_name = feature_premises[groups[g] + 1][0]
            feature_cardinality = groups[g + 1] - groups[g]
            categorical_feature_values[feature_name] = [feature_premises[groups[g] + 1 + k][1] for k in
                                                        range(feature_cardinality)]
        elif g == len(groups) - 2 and not feature_premises[groups[-2] + 1][1].isdigit():
            feature_name = feature_premises[groups[-1]][0]
            feature_cardinality = groups[-1] - groups[-2]
            categorical_feature_values[feature_name] = [feature_premises[groups[g] + 1 + k][1] for k in
                                                        range(feature_cardinality)]

    rules = []
    for corels_rule in corels_rules:
        parsed_corels_rule = corels_rule.replace(&#39;{y:&#39;, &#39;&#39;).replace(&#39;{&#39;, &#39;&#39;).replace(&#39;}&#39;, &#39;&#39;)\
                            .replace(&#39;(&#39;, &#39;&#39;).replace(&#39;)&#39;, &#39;&#39;).replace(&#39;if &#39;, &#39;&#39;).replace(&#39; then&#39;, &#39;&#39;).replace(&#39;else &#39;, &#39;&#39;)
        corels_premises = parsed_corels_rule.split(&#39; &#39;)
        output = int(corels_premises[-1])
        corels_premises = corels_premises[:-1]

        premises = {}
        # Last one is the default empty rule
        for corels_premise in corels_premises:
            corels_premise = corels_premise.replace(&#39;{&#39;, &#39;&#39;).replace(&#39;}&#39;, &#39;&#39;)

            # One-hot with positive value
            if &#39;:&#39; not in corels_premise:
                premises[int(corels_premise)] = (.5, +inf)
                continue

            feature, value = corels_premise.split(&#39;:&#39;)
            if value.isdigit() and int(value) in one_hot_names:
                feature, value = int(feature), int(value)
                idx = one_hot_names.index(feature if feature.isdigit() else feature)
                premises[idx] = tuple(undiscretize[feature][value])
            elif not value.isdigit() and feature + &#39;=&#39; + value in one_hot_names:
                # One-hot encoding
                idx = one_hot_names.index(feature + &#39;=&#39; + value)
                premises[idx] = 0.5, +inf
            else:
                idx = one_hot_names[int(feature)]
                premises[int(feature)] = tuple(undiscretize[idx][int(value)])

        rule = Rule(premises=premises, consequence=output)
        rules.append(rule)

    return rules</code></pre>
</details>
</dd>
<dt id="adversaries.cpar"><code class="name flex">
<span>def <span class="ident">cpar</span></span>(<span>x, y, path=None, cwd=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Train a CPAR on data x and labels y.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): The CPAR rules.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def cpar(x, y, path=None, cwd=None):
    &#34;&#34;&#34;Train a CPAR on data x and labels y.
    Arguments:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): The CPAR rules.
    &#34;&#34;&#34;
    if path is None:
        path = os.getcwd()
    if cwd is None:
        cwd = os.getcwd()

    df = DataFrame(hstack((x, y.reshape(-1, 1)))).convert_objects(convert_numeric=True)
    model = _cpar(df, path, cwd)
    rules = _rules_from_cpar(model)

    return rules</code></pre>
</details>
</dd>
<dt id="adversaries.dataset_to_corels_file"><code class="name flex">
<span>def <span class="ident">dataset_to_corels_file</span></span>(<span>df, output)</span>
</code></dt>
<dd>
<section class="desc"><p>Generate the binary output suitable for a CORELS run from
the given <code>df</code> in file <code>output</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>The data to binarize.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>str</code></dt>
<dd>The output file prefix. The output files
are <code>output</code>.data.corels and <code>output</code>.label.corels</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def dataset_to_corels_file(df, output):
    &#34;&#34;&#34;Generate the binary output suitable for a CORELS run from
    the given `df` in file `output`.
    Args:
        df (DataFrame): The data to binarize.
        output (str): The output file prefix. The output files
                        are `output`.data.corels and `output`.label.corels
    &#34;&#34;&#34;
    binaryzed_df = _binary_discretization(df, name=output)
    columns = binaryzed_df.columns
    descriptions = columns.tolist()[:-1]
    descriptions = [&#39;{&#39; + desc.replace(&#39;=&#39;, &#39;:&#39;) + &#39;} &#39; for desc in descriptions]
    values = [binaryzed_df[c].astype(int).astype(str) for c in columns[:-1]]
    feature_strings = [&#39; &#39;.join(feature_values) + &#39;\n&#39; for feature_values in values]
    feature_strings = [description + value for description, value in zip(descriptions, feature_strings)]
    feature_strings = &#39;&#39;.join(feature_strings)
    y = binaryzed_df.astype(int).values[:, -1]
    not_y = ((y + 1) % 2).astype(int)
    class_strings = [&#39;{y:1} &#39; + &#39; &#39;.join(y.astype(str)),
                     &#39;{y:0} &#39; + &#39; &#39;.join(not_y.astype(str))]
    class_strings = &#39;\n&#39;.join(class_strings)

    # Dump on disk
    with open(output + &#39;.data.corels&#39;, &#39;w&#39;) as log:
        log.write(feature_strings)
    with open(output + &#39;.label.corels&#39;, &#39;w&#39;) as log:
        log.write(class_strings)

    return feature_strings, class_strings</code></pre>
</details>
</dd>
<dt id="adversaries.dataset_to_sbrl_file"><code class="name flex">
<span>def <span class="ident">dataset_to_sbrl_file</span></span>(<span>df, output)</span>
</code></dt>
<dd>
<section class="desc"><p>Generate the binary output suitable for a SBRL run from
the given <code>df</code> in file <code>output</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>The data to binarize.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>str</code></dt>
<dd>The output file prefix. The output files
are <code>output</code>.data.sbrl and <code>output</code>.label.sbrl</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def dataset_to_sbrl_file(df, output):
    &#34;&#34;&#34;Generate the binary output suitable for a SBRL run from
    the given `df` in file `output`.
    Args:
        df (DataFrame): The data to binarize.
        output (str): The output file prefix. The output files
                        are `output`.data.sbrl and `output`.label.sbrl
    &#34;&#34;&#34;
    binaryzed_df = _binary_discretization(df, name=output)
    columns = binaryzed_df.columns
    descriptions = columns.tolist()[:-1]
    descriptions = [&#39;{&#39; + desc.replace(&#39;=&#39;, &#39;:&#39;) + &#39;} &#39; for desc in descriptions]
    values = [binaryzed_df[c].astype(int).astype(str) for c in columns[:-1]]
    feature_strings = [&#39; &#39;.join(feature_values) + &#39;\n&#39; for feature_values in values]
    feature_strings = [description + value for description, value in zip(descriptions, feature_strings)]
    feature_strings = &#39;&#39;.join(feature_strings)
    y = binaryzed_df.astype(int).values[:, -1]
    not_y = ((y + 1) % 2).astype(int)
    class_strings = [&#39;{y:1} &#39; + &#39; &#39;.join(y.astype(str)),
                     &#39;{y:0} &#39; + &#39; &#39;.join(not_y.astype(str))]
    class_strings = &#39;\n&#39;.join(class_strings)
    header = &#39;n_items: &#39; + str(df.shape[1]) + &#39;\n&#39;
    header = header + &#39;n_samples: &#39; + str(df.shape[0]) + &#39;\n&#39;
    header_labels = &#39;n_items: 2\n&#39;
    header_labels = header_labels + &#39;n_samples: &#39; + str(df.shape[0]) + &#39;\n&#39;

    # Dump on disk
    with open(output + &#39;.data.sbrl&#39;, &#39;w&#39;) as log:
        log.write(header + feature_strings)
    with open(output + &#39;.label.sbrl&#39;, &#39;w&#39;) as log:
        log.write(header_labels + class_strings)</code></pre>
</details>
</dd>
<dt id="adversaries.decision_set"><code class="name flex">
<span>def <span class="ident">decision_set</span></span>(<span>x, y)</span>
</code></dt>
<dd>
<section class="desc"><p>Train a Decision Set instance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): List of rules.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def decision_set(x, y):
    &#34;&#34;&#34;Train a Decision Set instance.
    Args:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of rules.
    &#34;&#34;&#34;
    decision_set  = DecisionSet()
    decision_set = decision_set.fit(x, y)
    rules = _rules_from_decision_set(decision_set)

    return rules</code></pre>
</details>
</dd>
<dt id="adversaries.decision_tree"><code class="name flex">
<span>def <span class="ident">decision_tree</span></span>(<span>x, y)</span>
</code></dt>
<dd>
<section class="desc"><p>Train a Decision Tree Classifier on x and y.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): List of <code>Rule</code>.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def decision_tree(x, y):
    &#34;&#34;&#34;Train a Decision Tree Classifier on x and y.
    Arguments:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of `Rule`.
    &#34;&#34;&#34;
    dt = DecisionTreeClassifier()
    dt.fit(x, y)
    rules = _rules_from_dt(dt)

    return rules</code></pre>
</details>
</dd>
<dt id="adversaries.foil"><code class="name flex">
<span>def <span class="ident">foil</span></span>(<span>x, y, path=None, cwd=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Train a FOIL on data x and labels y.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): The FOIL rules.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def foil(x, y, path=None, cwd=None):
    &#34;&#34;&#34;Train a FOIL on data x and labels y.
    Arguments:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): The FOIL rules.
    &#34;&#34;&#34;
    if path is None:
        path = os.getcwd()
    if cwd is None:
        cwd = os.getcwd()

    df = DataFrame(hstack((x, y.reshape(-1, 1)))).convert_objects(convert_numeric=True)
    model = _foil(df, path, cwd)
    rules = _rules_from_foil(model)

    return rules</code></pre>
</details>
</dd>
<dt id="adversaries.lore"><code class="name flex">
<span>def <span class="ident">lore</span></span>(<span>x, y, oracle)</span>
</code></dt>
<dd>
<section class="desc"><p>LORE adversary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): List of LORE rules.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def lore(x, y, oracle):
    &#34;&#34;&#34;LORE adversary.
    Args:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of LORE rules.
    &#34;&#34;&#34;
    # List of class values to map to integers
    # List of features, class excluded
    df = pd.DataFrame((hstack((x, y.reshape(-1, 1)))))
    df.columns = list(map(str, range(df.shape[1])))
    class_name = df.columns[-1]

    df, feature_names, class_values, numeric_columns, rdf, real_feature_names, features_map = prepare_dataset(df, class_name)
    # Constructor with class names, feature names and development data
    explainer = LOREM(x, oracle.predict, feature_names, class_name, class_values, numeric_columns,
                      features_map, neigh_type=&#39;rndgen&#39;, categorical_use_prob=True,
                      continuous_fun_estimation=False, size=750, ocr=0.1, multi_label=False, one_vs_rest=False,
                      verbose=False, ngen=10)

    # Extract anchors
    rules = []
    for i in tqdm(range(x.shape[0])):
        explanation = explainer.explain_instance(x[i], use_weights=True, metric=&#39;euclidean&#39;)
        rule = _rule_from_lore(explanation)
        rules.append(rule)

    return rules</code></pre>
</details>
</dd>
<dt id="adversaries.pruned_decision_tree"><code class="name flex">
<span>def <span class="ident">pruned_decision_tree</span></span>(<span>x, y, max_depth=4)</span>
</code></dt>
<dd>
<section class="desc"><p>Train a Decision Tree Classifier on <code>x</code> and <code>y</code> with maximum depth <code>max_depth</code>.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): List of <code>Rule</code>.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def pruned_decision_tree(x, y, max_depth=4):
    &#34;&#34;&#34;Train a Decision Tree Classifier on `x` and `y` with maximum depth `max_depth`.
    Arguments:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (list): List of `Rule`.
    &#34;&#34;&#34;
    dt = DecisionTreeClassifier(max_depth=max_depth)
    dt.fit(x, y)
    rules = _rules_from_dt(dt)

    return rules</code></pre>
</details>
</dd>
<dt id="adversaries.trepan"><code class="name flex">
<span>def <span class="ident">trepan</span></span>(<span>x, y, oracle)</span>
</code></dt>
<dd>
<section class="desc"><p>Train a Trepan instance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
<dt><strong><code>oracle</code></strong> :&ensp;<code>Predictor</code></dt>
<dd>The oracle used to grow the train data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(list): List of rules.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def trepan(x, y, oracle):
    &#34;&#34;&#34;Train a Trepan instance.
    Args:
        x (ndarray): Data.
        y (ndarray): Labels.
        oracle (Predictor): The oracle used to grow the train data.
    Returns:
        (list): List of rules.
    &#34;&#34;&#34;
    trepan = TREPAN(oracle)
    data = pd.DataFrame(hstack((x, y.reshape(-1, 1))))
    trepan = trepan.fit(data, max_nodes=3)
    rules = _rule_from_trepan(trepan)

    return rules</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="adversaries.CPAR"><code class="flex name class">
<span>class <span class="ident">CPAR</span></span>
</code></dt>
<dd>
<section class="desc"><p>CPAR classifier.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class CPAR:
    &#34;&#34;&#34;CPAR classifier.&#34;&#34;&#34;

    def __init__(self, cpar, encoders, discretizer, scaling_factors, types, path, cwd):
        &#34;&#34;&#34;Constructor.
        Arguments:
            cpar: CPAR instance.
            encoders (list): Encoders.
            discretizer (BaseDiscretizer): Discretizer to scale data for CPAR.
            scaling_factors (iterable): Scaling factors to scale data for CPAR.
            types (iterable): Data types.
            path (str): Working directory.
            cwd (str): Current working directory.
        &#34;&#34;&#34;
        self.cpar = cpar
        self.encoders = encoders
        self.discretizer = discretizer
        self.scaling_factors = scaling_factors
        self.types = types
        self.path = path
        self.cwd = cwd

    def predict(self, x):
        &#34;&#34;&#34;Predict x.
        Arguments:
            x (ndarray): Data to predict.
        Returns:
            (ndarray): Predictions on x.
        &#34;&#34;&#34;
        x_ = x.copy()
        os.chdir(self.path)
        x_ = _decode_from_jrbc(DataFrame(x_), self.encoders, self.discretizer, self.scaling_factors)
        y = self.cpar.predict(x_)
        scaled_y = y - self.scale[-1]
        os.chdir(self.cwd)

        # Fix for missing values in the TR
        scaled_y -= min(scaled_y)

        return scaled_y</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="adversaries.CPAR.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, cpar, encoders, discretizer, scaling_factors, types, path, cwd)</span>
</code></dt>
<dd>
<section class="desc"><p>Constructor.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><a title="adversaries.cpar" href="#adversaries.cpar"><code>cpar()</code></a></strong></dt>
<dd>CPAR instance.</dd>
<dt><strong><code>encoders</code></strong> :&ensp;<code>list</code></dt>
<dd>Encoders.</dd>
<dt><strong><code>discretizer</code></strong> :&ensp;<code>BaseDiscretizer</code></dt>
<dd>Discretizer to scale data for CPAR.</dd>
<dt><strong><code>scaling_factors</code></strong> :&ensp;<code>iterable</code></dt>
<dd>Scaling factors to scale data for CPAR.</dd>
<dt><strong><code>types</code></strong> :&ensp;<code>iterable</code></dt>
<dd>Data types.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Working directory.</dd>
<dt><strong><code>cwd</code></strong> :&ensp;<code>str</code></dt>
<dd>Current working directory.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, cpar, encoders, discretizer, scaling_factors, types, path, cwd):
    &#34;&#34;&#34;Constructor.
    Arguments:
        cpar: CPAR instance.
        encoders (list): Encoders.
        discretizer (BaseDiscretizer): Discretizer to scale data for CPAR.
        scaling_factors (iterable): Scaling factors to scale data for CPAR.
        types (iterable): Data types.
        path (str): Working directory.
        cwd (str): Current working directory.
    &#34;&#34;&#34;
    self.cpar = cpar
    self.encoders = encoders
    self.discretizer = discretizer
    self.scaling_factors = scaling_factors
    self.types = types
    self.path = path
    self.cwd = cwd</code></pre>
</details>
</dd>
<dt id="adversaries.CPAR.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict x.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data to predict.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(ndarray): Predictions on x.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def predict(self, x):
    &#34;&#34;&#34;Predict x.
    Arguments:
        x (ndarray): Data to predict.
    Returns:
        (ndarray): Predictions on x.
    &#34;&#34;&#34;
    x_ = x.copy()
    os.chdir(self.path)
    x_ = _decode_from_jrbc(DataFrame(x_), self.encoders, self.discretizer, self.scaling_factors)
    y = self.cpar.predict(x_)
    scaled_y = y - self.scale[-1]
    os.chdir(self.cwd)

    # Fix for missing values in the TR
    scaled_y -= min(scaled_y)

    return scaled_y</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="adversaries.DecisionSet"><code class="flex name class">
<span>class <span class="ident">DecisionSet</span></span>
</code></dt>
<dd>
<section class="desc"><p>Decision set instance.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class DecisionSet:
    &#34;&#34;&#34;Decision set instance.&#34;&#34;&#34;

    def __init__(self):
        self.rules = None

    def fit(self, x, y):
        &#34;&#34;&#34;Train this instance.
        Args:
            x (ndarray): Data.
            y (ndarray): Labels.
        Returns:
            (DecisionSet): Returns `self`.
        &#34;&#34;&#34;
        return None</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="adversaries.DecisionSet.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize self.
See help(type(self)) for accurate signature.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self):
    self.rules = None</code></pre>
</details>
</dd>
<dt id="adversaries.DecisionSet.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<section class="desc"><p>Train this instance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Labels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(DecisionSet): Returns <code>self</code>.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit(self, x, y):
    &#34;&#34;&#34;Train this instance.
    Args:
        x (ndarray): Data.
        y (ndarray): Labels.
    Returns:
        (DecisionSet): Returns `self`.
    &#34;&#34;&#34;
    return None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="adversaries.FOIL"><code class="flex name class">
<span>class <span class="ident">FOIL</span></span>
</code></dt>
<dd>
<section class="desc"><p>FOIL classifier.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class FOIL:
    &#34;&#34;&#34;FOIL classifier.&#34;&#34;&#34;

    def __init__(self, foil, encoders, discretizer, scaling_factors, types, path, cwd):
        &#34;&#34;&#34;Constructor.
        Arguments:
            cpar: FOIL instance.
            encoders (list): Encoders.
            discretizer (BaseDiscretizer): Discretizer to scale data for FOIL.
            scaling_factors (iterable): Scaling factors to scale data for FOIL.
            types (iterable): Data types.
            path (str): Working directory.
            cwd (str): Current working directory.
        &#34;&#34;&#34;
        self.foil = foil
        self.encoders = encoders
        self.discretizer = discretizer
        self.scaling_factors = scaling_factors
        self.types = types
        self.path = path
        self.cwd = cwd

    def predict(self, x):
        &#34;&#34;&#34;Predict x.
        Arguments:
            x (ndarray): Data to predict.
        Returns:
            (ndarray): Predictions on x.
        &#34;&#34;&#34;
        x_ = x.copy()
        os.chdir(self.path)

        for i, scale in enumerate(self.scale[:-1]):
            x_[:, i] += scale

        y = self.foil.predict(x_)
        scaled_y = y - self.scale[-1]
        os.chdir(self.cwd)

        # Fix for missing values in the TR
        scaled_y -= min(scaled_y)

        return scaled_y</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="adversaries.FOIL.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, foil, encoders, discretizer, scaling_factors, types, path, cwd)</span>
</code></dt>
<dd>
<section class="desc"><p>Constructor.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><a title="adversaries.cpar" href="#adversaries.cpar"><code>cpar()</code></a></strong></dt>
<dd>FOIL instance.</dd>
<dt><strong><code>encoders</code></strong> :&ensp;<code>list</code></dt>
<dd>Encoders.</dd>
<dt><strong><code>discretizer</code></strong> :&ensp;<code>BaseDiscretizer</code></dt>
<dd>Discretizer to scale data for FOIL.</dd>
<dt><strong><code>scaling_factors</code></strong> :&ensp;<code>iterable</code></dt>
<dd>Scaling factors to scale data for FOIL.</dd>
<dt><strong><code>types</code></strong> :&ensp;<code>iterable</code></dt>
<dd>Data types.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Working directory.</dd>
<dt><strong><code>cwd</code></strong> :&ensp;<code>str</code></dt>
<dd>Current working directory.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, foil, encoders, discretizer, scaling_factors, types, path, cwd):
    &#34;&#34;&#34;Constructor.
    Arguments:
        cpar: FOIL instance.
        encoders (list): Encoders.
        discretizer (BaseDiscretizer): Discretizer to scale data for FOIL.
        scaling_factors (iterable): Scaling factors to scale data for FOIL.
        types (iterable): Data types.
        path (str): Working directory.
        cwd (str): Current working directory.
    &#34;&#34;&#34;
    self.foil = foil
    self.encoders = encoders
    self.discretizer = discretizer
    self.scaling_factors = scaling_factors
    self.types = types
    self.path = path
    self.cwd = cwd</code></pre>
</details>
</dd>
<dt id="adversaries.FOIL.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict x.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Data to predict.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(ndarray): Predictions on x.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def predict(self, x):
    &#34;&#34;&#34;Predict x.
    Arguments:
        x (ndarray): Data to predict.
    Returns:
        (ndarray): Predictions on x.
    &#34;&#34;&#34;
    x_ = x.copy()
    os.chdir(self.path)

    for i, scale in enumerate(self.scale[:-1]):
        x_[:, i] += scale

    y = self.foil.predict(x_)
    scaled_y = y - self.scale[-1]
    os.chdir(self.cwd)

    # Fix for missing values in the TR
    scaled_y -= min(scaled_y)

    return scaled_y</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="adversaries.anchors" href="#adversaries.anchors">anchors</a></code></li>
<li><code><a title="adversaries.bayesian_rule_lists" href="#adversaries.bayesian_rule_lists">bayesian_rule_lists</a></code></li>
<li><code><a title="adversaries.corels" href="#adversaries.corels">corels</a></code></li>
<li><code><a title="adversaries.cpar" href="#adversaries.cpar">cpar</a></code></li>
<li><code><a title="adversaries.dataset_to_corels_file" href="#adversaries.dataset_to_corels_file">dataset_to_corels_file</a></code></li>
<li><code><a title="adversaries.dataset_to_sbrl_file" href="#adversaries.dataset_to_sbrl_file">dataset_to_sbrl_file</a></code></li>
<li><code><a title="adversaries.decision_set" href="#adversaries.decision_set">decision_set</a></code></li>
<li><code><a title="adversaries.decision_tree" href="#adversaries.decision_tree">decision_tree</a></code></li>
<li><code><a title="adversaries.foil" href="#adversaries.foil">foil</a></code></li>
<li><code><a title="adversaries.lore" href="#adversaries.lore">lore</a></code></li>
<li><code><a title="adversaries.pruned_decision_tree" href="#adversaries.pruned_decision_tree">pruned_decision_tree</a></code></li>
<li><code><a title="adversaries.trepan" href="#adversaries.trepan">trepan</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="adversaries.CPAR" href="#adversaries.CPAR">CPAR</a></code></h4>
<ul class="">
<li><code><a title="adversaries.CPAR.__init__" href="#adversaries.CPAR.__init__">__init__</a></code></li>
<li><code><a title="adversaries.CPAR.predict" href="#adversaries.CPAR.predict">predict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="adversaries.DecisionSet" href="#adversaries.DecisionSet">DecisionSet</a></code></h4>
<ul class="">
<li><code><a title="adversaries.DecisionSet.__init__" href="#adversaries.DecisionSet.__init__">__init__</a></code></li>
<li><code><a title="adversaries.DecisionSet.fit" href="#adversaries.DecisionSet.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="adversaries.FOIL" href="#adversaries.FOIL">FOIL</a></code></h4>
<ul class="">
<li><code><a title="adversaries.FOIL.__init__" href="#adversaries.FOIL.__init__">__init__</a></code></li>
<li><code><a title="adversaries.FOIL.predict" href="#adversaries.FOIL.predict">predict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>